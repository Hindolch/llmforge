# ğŸš€ LLMForge

**LLMForge** is a modular LLMOps pipeline originally designed for Reddit-based dataset curation and LoRA fine-tuning on `TinyLLaMA-1.1B`. It automates ingestion, cleaning, training, and Hugging Face syncing â€” all backed by `Prefect`, `Modal`, and CI/CD best practices.

---

## âœ… Whatâ€™s New (June 2025)

Weâ€™ve added **a one-line fine-tuning CLI flow** using Modal GPU compute and any Hugging Face dataset.

```bash
./llmforge finetune \
  --dataset-repo yourusername/your-dataset \
  --model-repo yourusername/your-model-name \
  --hf-auth ./hf_token.txt \
  --push-to-hub
````

No setup beyond a HF token file and basic venv install. Full walkthrough and screenshots below ğŸ‘‡

---

## ğŸ§  Why This Update Matters

This update streamlines LLMForge into a **modular LoRA trainer** for the OSS community:

* ğŸ”¥ No MLOps knowledge needed to fine-tune & push your own model
* ğŸ§¼ Old full-pipeline logic is retained but commented for now
* ğŸ” Fast launch, easy to build on top of

---

## âš™ï¸ Key Features

* ğŸ” Reddit-based prompt-completion dataset creation
* ğŸ§¹ Toxicity filtering using Detoxify
* âš™ï¸ LoRA fine-tuning on Modal with GPU (`T4`)
* â˜ï¸ Hugging Face Dataset + Model Hub syncing
* ğŸ” `Prefect` orchestration pipeline (legacy)
* ğŸš€ New! One-command fine-tuning via CLI + Modal
* ğŸ§ª CI setup with `pytest` and GitHub Actions
* ğŸ“§ Email alerts for pipeline (optional)

---

## ğŸ†• New: Simple Finetune CLI

After cloning the repo:

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

Place your Hugging Face token in a file named `hf_token.txt`.

Then run:

```bash
./llmforge finetune \
  --dataset-repo yourusername/your-dataset(whatever dataset repo name you want to give) \
  --model-repo yourusername/your-model-name(same naming conventions like dataset) \
  --hf-auth ./hf_token.txt \
  --push-to-hub
```

That's it. Modal will handle the GPU job and push the model to your HF.

---

## ğŸ§¼ Notes on Code Cleanup

* `tinylama` and old full-pipeline fine-tuning logic are **commented** (not deleted).
* The 12-hour looped pipeline is **inactive**, but logic is retained for future revival.
* `modal secret` creation is handled dynamically using your HF token.

---

## ğŸ”§ Architecture Overview

| Module     | Role                                    |
| ---------- | --------------------------------------- |
| `src/`     | Pipeline + finetuning logic             |
| `tasks/`   | Prefect-wrapped tasks for orchestration |
| `tests/`   | Test coverage                           |
| `llmforge` | CLI to trigger training                 |

---

### ğŸ› ï¸ Features Implemented

| Feature                        | Status |
| ------------------------------ | ------ |
| Reddit Ingestion with PRAW     | âœ…      |
| Toxicity Filtering (Detoxify)  | âœ…      |
| Prompt-Completion Generation   | âœ…      |
| Hugging Face Dataset Push      | âœ…      |
| LoRA Fine-tuning on Modal      | âœ…      |
| Hugging Face Model Push        | âœ…      |
| CLI-based Finetune Trigger     | âœ…      |
| CI + `pytest` test integration | âœ…      |
| Email Alerts                   | âœ…      |
| Streamlit Inference UI (local) | âœ…      |

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ hf_uploader.py
â”œâ”€â”€ inference.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ llmforge                   # CLI wrapper for launching fine-tune jobs
â”œâ”€â”€ prefect.yaml
â”œâ”€â”€ __pycache__
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_processor.py
â”‚   â”œâ”€â”€ email.py
â”‚   â”œâ”€â”€ finetune.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ __pycache__
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ data_ingestion_task.py
â”‚   â”œâ”€â”€ data_processor_task.py
â”‚   â”œâ”€â”€ finetune_task.py
â”‚   â””â”€â”€ __pycache__
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_pipeline.py
â”œâ”€â”€ venv
â””â”€â”€

---

### ğŸ“· Screenshots

#### ğŸ”„ One-command Modal GPU Job

![modal\_job](https://github.com/user-attachments/assets/74a9533e-0a70-4cda-a368-5f9862588a57)

#### âœ… Model Pushed to Hugging Face

![hf\_model\_push](https://github.com/user-attachments/assets/f7c61de8-9ed0-449c-9dee-1ebb08f3ad8d)

#### ğŸ§ª CI + Prefect Pipelines

![ci\_pass](https://github.com/user-attachments/assets/e563a12f-064f-4998-a0f2-98d92e99ca50)
![prefect\_worker](https://github.com/user-attachments/assets/936881dd-9d21-4afa-8e6b-56d6922e4405)

#### ğŸ›ï¸ Streamlit Inference UI

![streamlit](https://github.com/user-attachments/assets/0a60489a-4eaf-4d32-974f-30869d95f0ec)

---

## ğŸ§ª Testing

```bash
pytest tests/
```

âœ… All tests pass for ETL logic and CLI triggers

---

### âš ï¸ Dev Notes

LLMForge was built on a **4GB GPU laptop**:

* Modal handles all remote fine-tuning
* The 12-hr pipeline is **currently disabled** for cost reasons
* Adapter merging was skipped for memory savings

Despite that:

* Itâ€™s cloud-ready and reproducible
* Works on real Reddit + HF datasets
* Fully OSS and tweakable

---

## ğŸ§  Future Extensions

* [ ] Merge adapters for complete model export
* [ ] Auto-deploy Streamlit via Modal or Render
* [ ] Reactivate scheduled flows (Prefect)
* [ ] Add Rouge/BLEU scoring post-finetune
* [ ] Support data balancing + multi-source ingestion

---

## ğŸ§‘â€ğŸ’» Author

**Hindol R. Choudhury**
*MLOps â€¢ LLM Infra â€¢ Applied AI*
ğŸ“« [LinkedIn](https://www.linkedin.com/in/hindol-choudhury/)

---

```
