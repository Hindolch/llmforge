### ğŸš€ LLMForge

**LLMForge** is a modular LLMOps pipeline for Reddit-based data curation and LoRA fine-tuning on `TinyLLaMA-1.1B` every 12 hours. Designed to reflect real-world MLOps orchestration, it automates ingestion, cleaning, training, and Hugging Face syncing â€” all backed by `Prefect`, `Modal`, and clean CI/CD practices.

---

## âœ… Key Features

- ğŸ” Reddit-based prompt-completion dataset creation
- ğŸ§¹ Toxicity filtering using Detoxify
- âš™ï¸ Fine-tuning TinyLLaMA with PEFT + LoRA adapters
- â˜ï¸ Hugging Face Dataset + Model Hub uploads
- ğŸ” Fully orchestrated using `Prefect` flows and tasks
- ğŸš€ GPU-powered training on Modal (`T4` instance)
- ğŸ§ª CI pipeline with `pytest` and GitHub Actions
- ğŸ“§ Email alerts on pipeline trigger
- â± The whole workflow from start to finsh is being done every 12 hours as said above. (`Fully Automated`) 

---

## ğŸ§  Why This Matters

Built with an engineer's mindset under tight resource constraints, LLMForge shows:

- ğŸ’¡ Real-world thinking in orchestrated MLOps
- ğŸ¯ Efficient LoRA fine-tuning without overfitting
- ğŸ§¼ NLP-focused cleaning pipelines that reduce noise
- âš™ï¸ Cloud-first thinking (Modal + Prefect + Hugging Face)

---

## ğŸ”§ Architecture Overview

| Module             | Role                                                  |
|-------------------|-------------------------------------------------------|
| `src/`            | All pipeline logic (ingestion, processing, finetune) |
| `tasks/`          | Prefect-wrapped tasks for orchestration              |
| `tests/`          | Basic CI-compatible test coverage                    |
| `modal`           | Runs remote GPU training using fine-tuned HF dataset |
| `prefect.yaml`    | Contains deployment metadata for Prefect Cloud       |

---

### ğŸ› ï¸ Features Implemented

| Feature                          | Status |
|----------------------------------|--------|
| Reddit Ingestion with PRAW       | âœ…     |
| Toxicity Filtering (Detoxify)    | âœ…     |
| Prompt-Completion Generation     | âœ…     |
| Hugging Face Dataset Push        | âœ…     |
| LoRA Fine-tuning on Modal        | âœ…     |
| Hugging Face Model Push          | âœ…     |
| Prefect Workflow Automation      | âœ…     |
| CI + `pytest` test integration   | âœ…     |
| Email Alerts per pipeline run    | âœ…     |
| Streamlit Inference UI (local)   | âœ…     |

---

### âš™ï¸ Prefect Orchestration

- âœ… This project was deployed on **Prefect Cloud** using `prefect.yaml`
- ğŸ” A **worker pool** is configured, enabling scalable background runs
- âŒ However, due to budget constraints, periodic scheduling is disabled

ğŸ’¡ Despite system limitations, the orchestration is ready for production use.

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ run_pipeline.py            # Main flow trigger
â”œâ”€â”€ inference.py               # (Optional) Streamlit-based inference
â”œâ”€â”€ download_model.py          # HF pull helper
â”œâ”€â”€ prefect.yaml               # Prefect deployment metadata
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_processor.py
â”‚   â”œâ”€â”€ email.py
â”‚   â””â”€â”€ finetune.py
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ data_ingestion_task.py
â”‚   â”œâ”€â”€ data_processor_task.py
â”‚   â””â”€â”€ finetune_task.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py
â””â”€â”€ .env.example               # Add your creds here
````

---

### ğŸ“· Screenshots


### Prefect Flow, Worker Pool & Pipeline Deployment
![Screenshot from 2025-06-30 12-45-19](https://github.com/user-attachments/assets/0af1526f-1020-40a5-baf7-108b4610da67)
![Screenshot from 2025-06-30 18-52-34](https://github.com/user-attachments/assets/673dc554-a1df-4c11-ae13-553ab6c4f325)
![Screenshot from 2025-06-30 18-52-12](https://github.com/user-attachments/assets/936881dd-9d21-4afa-8e6b-56d6922e4405)

### Modal UI of the Job
![Screenshot from 2025-06-30 19-36-05](https://github.com/user-attachments/assets/74a9533e-0a70-4cda-a368-5f9862588a57)

### HF Fine-Tuned Model Update & Dataset 
![Screenshot from 2025-06-30 18-05-28](https://github.com/user-attachments/assets/f7c61de8-9ed0-449c-9dee-1ebb08f3ad8d)

### CI: GitHub Actions passing 
![Screenshot from 2025-06-30 18-59-53](https://github.com/user-attachments/assets/e563a12f-064f-4998-a0f2-98d92e99ca50)

### Streamlit Inference UI
![Screenshot from 2025-06-30 18-30-01](https://github.com/user-attachments/assets/0a60489a-4eaf-4d32-974f-30869d95f0ec)

### Email Notification
![Screenshot from 2025-06-30 18-37-01](https://github.com/user-attachments/assets/330a8303-ae8a-4ce8-88d6-a448a3e2e35c)


## ğŸ§ª Testing

```bash
pytest tests/
```

âœ… All test cases pass (including basic ETL & pipeline stub validation)

---

### âš ï¸ Dev Notes on System Constraints

LLMForge was built entirely on a **4GB GPU laptop**, hence:

* Streamlit app was tested locally, not deployed
* Prefect's recurring schedules were not activated
* Adapter merging was skipped to save VRAM

Despite that, the entire pipeline is:

* Modular, scalable, and cleanly orchestrated
* Cloud-ready: can run on GPU infra + Prefect Cloud + HF Hub

---

## ğŸ§  Future Extensions

* [ ] Merge LoRA adapters for full model export
* [ ] Cloud host the Streamlit UI (via Modal or Render)
* [ ] Add cron-like retriggers using Prefect schedules
* [ ] Support multi-subreddit ingestion and balancing
* [ ] Evaluate and log Rouge/Loss metrics post fine-tune

---

## ğŸ§‘â€ğŸ’» Author

**Hindol R. Choudhury**
*MLOps â€¢ LLM Infra â€¢ Applied AI*
ğŸ“« [LinkedIn](https://www.linkedin.com/in/hindol-choudhury/)


---

